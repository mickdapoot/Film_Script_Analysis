{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickdapoot/Film_Script_Analysis/blob/master/RuvPROMPT5dd85664f2003d0367269e3e3ca5a576.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkOfLzwuZmOf"
      },
      "source": [
        "## rUv Metaprompt Generator\n",
        "\n",
        "ğŸª° Created by @rUv\n",
        "\n",
        "Welcome to the **Metaprompt Generator** - a high-level tool designed to facilitate the creation of effective, task-specific prompts that enhance the capabilities of AI models across diverse applications. This tool serves as a cornerstone in advancing how artificial intelligence understands and processes human directives, ensuring more meaningful and contextually aware interactions.\n",
        "\n",
        "### What are MetaPrompts?\n",
        "A metaprompt is an advanced approach to prompt engineering that optimizes the performance of artificial intelligence systems by providing a framework that shapes how AI models process and respond to tasks. Essentially, it serves as a blueprint for generating specific, lower-level prompts that direct AI in performing a wide array of functions, from simple data retrieval to complex problem-solving.\n",
        "\n",
        "The significance of metaprompts lies in their ability to enhance the AI's understanding of context and intent, thereby improving the accuracy and relevance of its outputs. This is crucial in environments where the precision of AI responses can dramatically impact decision-making processes and outcomes.\n",
        "\n",
        "Metaprompts are not only foundational in guiding AI interactions but are also inherently flexible, allowing developers to tailor them according to the unique demands of different applications. By modifying a metaprompt, developers can adjust the depth of inquiry, the tone of interaction, and the complexity of the tasks AI models are expected to execute.\n",
        "\n",
        "This adaptability makes metaprompts invaluable in sectors ranging from customer service, where they can refine chatbot interactions, to advanced research fields, where they can guide AI in synthesizing information or generating insights based on large datasets. Thus, metaprompts represent a pivotal development in the field of AI, pushing the boundaries of what machines can understand and accomplish while ensuring they operate within defined parameters that align with human expectations and needs.\n",
        "\n",
        "### Key Features of the Metaprompt Generator:\n",
        "\n",
        "1. **Dynamic Prompt Structuring:** It equips AI with the ability to generate dynamic, structured prompts that are tailored to the specific needs of various tasks. This adaptability is crucial for applications ranging from straightforward question answering to complex analytical problem-solving.\n",
        "\n",
        "2. **Wide Range of Applications:** The generator's versatility covers an extensive array of tasks including customer support, sentence comparison, document-based question answering, math problem-solving, and execution of specialized functions. This breadth ensures that the AI can function effectively in diverse scenarios.\n",
        "\n",
        "3. **Guided AI Reasoning:** By incorporating techniques such as inner monologues and scratchpads, the metaprompt helps guide the AI's reasoning process. This structured guidance aids the AI in developing a deeper understanding of the tasks and formulating appropriate responses.\n",
        "\n",
        "4. **Template-Driven Approach:** The system uses a template-driven approach where each task is presented with a clear set of examples showing well-structured prompts. This helps set a high standard for what effective AI prompts should look like.\n",
        "\n",
        "5. **Customizable Task Inputs:** Users can customize the AI's task by replacing the \"{{TASK}}\" placeholder within the metaprompt. This feature allows users to specify the exact nature of the task, making the tool highly adaptable to user needs.\n",
        "\n",
        "6. **Detailed Instructional Design:** Each prompt is crafted to provide detailed, step-by-step instructions that the AI follows. This includes identifying minimal input variables, planning out the prompt structure, and writing out detailed instructions that mimic successful examples.\n",
        "\n",
        "7. **Integration and Compatibility:** The generator is designed to seamlessly integrate with existing AI frameworks, supporting a range of AI models and architectures. It also allows users to specify various parameters that influence prompt generation, such as verbosity, context scope, and technical constraints.\n",
        "\n",
        "8. **Innovative Enhancements:** The project incorporates cutting-edge algorithms that analyze task characteristics, optimize prompt complexity, and ensure that prompts are both effective and efficient.\n",
        "\n",
        "### MetaPrompt Templates\n",
        "These templates serve as starting points that can be extensively customized to meet the nuanced demands of different AI applications, making them a versatile tool in the development of intelligent systems. Each templateâ€™s parameters and focus areas can be adjusted, providing a tailored approach that aligns with the specific goals and constraints of the project at hand.\n",
        "\n",
        "- **Generic Metaprompt Generator**:\n",
        "  - **Use**: To create adaptive prompts that guide AI models in diverse applications, enhancing their contextual understanding and interaction quality.\n",
        "  - **Modification**: Adjust complexity and specificity of the prompts based on the task's domain, incorporating varying levels of detail and technical terminology as needed.\n",
        "\n",
        "- **Advanced Coding System**:\n",
        "  - **Use**: Tailored for optimizing software development processes through AI-driven code refinement and optimization techniques.\n",
        "  - **Modification**: Integrate specific programming languages or frameworks, and adjust the AIâ€™s focus on particular aspects of coding, such as debugging or performance improvements.\n",
        "\n",
        "- **Autonomous Agents**:\n",
        "  - **Use**: Designed to enable AI agents to make decisions and learn autonomously within dynamic environments, such as robotics or automated systems.\n",
        "  - **Modification**: Customize learning parameters, environmental variables, and the decision-making model to fit specific operational contexts or safety requirements.\n",
        "\n",
        "- **Agentic Employees**:\n",
        "  - **Use**: Enhances employee decision-making agency by providing AI-supported tools that aid in complex decision processes.\n",
        "  - **Modification**: Modify to focus on particular types of decisions, such as strategic business choices or HR policies, tailoring the AIâ€™s insights to sector-specific data.\n",
        "\n",
        "- **Problem Solver**:\n",
        "  - **Use**: Aids AI in tackling multi-dimensional, complex problems across various industries by generating structured problem-solving prompts.\n",
        "  - **Modification**: Focus the problem solver on specific industries like healthcare or finance, adjusting the types of data it analyzes and the methodologies it employs.\n",
        "\n",
        "\n",
        "### Get Started:\n",
        "\n",
        "To begin using the Metaprompt Generator, select a task template or enter your specific task requirements, and let the AI generate a tailored prompt that guides the AI model to accomplish the task effectively. Explore the potential of AI with structured, precise, and context-aware prompts that make AI interactions more intuitive and result-oriented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4H2RYFs8Zhqa",
        "outputId": "640799a3-03cf-4db3-ccc0-1b46982d580f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litellm\n",
            "  Downloading litellm-1.37.5-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (7.1.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.4)\n",
            "Collecting openai>=1.0.0 (from litellm)\n",
            "  Downloading openai-1.28.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.2.0 (from litellm)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.31.0)\n",
            "Collecting tiktoken>=0.4.0 (from litellm)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.19.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->litellm) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.0.0->litellm)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->litellm) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.20.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->litellm) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0->litellm)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->litellm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->litellm) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->litellm) (2.18.2)\n",
            "Installing collected packages: python-dotenv, h11, tiktoken, httpcore, httpx, openai, litellm\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 litellm-1.37.5 openai-1.28.2 python-dotenv-1.0.1 tiktoken-0.7.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.31.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.2 (from gradio)\n",
            "  Downloading gradio_client-0.16.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.2->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.2->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=44d1b510faf7599f5a1832170497ad9ec2875b1019eb1f79f1244f191d95a3fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, uvicorn, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, httptools, dnspython, aiofiles, watchfiles, starlette, email_validator, typer, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 ffmpy-0.3.2 gradio-4.31.0 gradio-client-0.16.2 httptools-0.6.1 orjson-3.10.3 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "# @title Install Libraries\n",
        "!pip install litellm\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXh2p8HfZvsw"
      },
      "outputs": [],
      "source": [
        "# @title Configure and Set API KEY\n",
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the OpenAI API key using the value stored in Colab secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Verify the API key is set\n",
        "if 'OPENAI_API_KEY' in os.environ:\n",
        "    print(\"OpenAI API key is set as an environment variable. Ready to proceed!\")\n",
        "else:\n",
        "    print(\"OpenAI API key is not set as an environment variable. Please check your setup.\")\n",
        "\n",
        "MODEL_NAME = \"gpt-4-32k-0314\"  # Set the desired OpenAI model name here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_9OIi8UZyJH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title MetaPrompt Creator v1.\n",
        "import gradio as gr\n",
        "import litellm\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "import requests\n",
        "\n",
        "MODEL_NAME = \"gpt-4-32k-0314\"  # Set the desired OpenAI model name here\n",
        "\n",
        "# Define the templates with default values for each field\n",
        "templates = {\n",
        "    \"Generic Metaprompt Generator\": {\n",
        "        \"task\": \"\"\"Develop a sophisticated and adaptable metaprompt generator designed to significantly enhance the capabilities of artificial intelligence models across various domains. The objective of this generator is not merely to aid in task execution but to fundamentally transform how AI interprets and interacts with human inputs, ensuring a deeper contextual understanding and more nuanced responses.\n",
        "          This system should cater to a wide range of applicationsâ€”from simple query resolution to complex problem-solving scenariosâ€”by dynamically generating tailored prompts that guide AI models to achieve optimal performance. The generator should incorporate advanced algorithms that analyze the nature of tasks, discern the required depth and specificity of responses, and adjust the complexity of prompts accordingly.\n",
        "          Furthermore, the system is expected to seamlessly integrate with existing AI frameworks, providing an intuitive interface for users to specify parameters that influence prompt generation. These parameters may include the desired verbosity, the scope of context consideration, specific linguistic styles, and technical constraints that ensure compatibility with various AI model architectures.\n",
        "          By executing this project, the metaprompt generator will become a pivotal tool in enhancing the effectiveness of AI deployments, ensuring that AI systems can perform with greater autonomy while maintaining alignment with human expectations and operational standards. This initiative is anticipated to set new benchmarks in AI utility, pushing the boundaries of what is possible through enhanced human-AI synergy.\n",
        "        \"\"\",\n",
        "        \"faq\": \"What is a metaprompt and how does it improve AI performance?\",\n",
        "        \"company\": \"Generic Co., a leader in AI technology deployment.\",\n",
        "        \"inquiry\": \"Could you explain the starting steps for creating effective metaprompts?\",\n",
        "        \"math_question\": \"\",\n",
        "        \"sentence1\": \"Metaprompts are essential tools in structuring the interaction between humans and AI.\",\n",
        "        \"sentence2\": \"These tools ensure that AI understands and executes tasks with high precision.\",\n",
        "        \"question\": \"Can you delineate the essential components that make a metaprompt successful?\",\n",
        "        \"document\": \"\",\n",
        "        \"problem\": \"The challenge is to maintain flexibility while ensuring that metaprompts are robust across different AI scenarios.\",\n",
        "        \"functions\": \"generate_metaprompt(task, details)\"\n",
        "    },\n",
        "    \"Advanced Coding System\": {\n",
        "        \"task\": \"Develop a cutting-edge coding system that leverages software optimization techniques to enhance performance and efficiency.\",\n",
        "        \"faq\": \"What exactly does software optimization entail, and why is it critical for modern software development?\",\n",
        "        \"company\": \"Tech Innovations Ltd., known for its pioneering software solutions.\",\n",
        "        \"inquiry\": \"What are the most recent advancements in software optimization technologies?\",\n",
        "        \"math_question\": \"\",\n",
        "        \"sentence1\": \"Software optimization is a critical step in refining code to enhance execution speed and resource management.\",\n",
        "        \"sentence2\": \"The use of AI in advanced coding systems can significantly accelerate the development cycles and improve code quality.\",\n",
        "        \"question\": \"How can artificial intelligence be utilized to automate and optimize coding processes?\",\n",
        "        \"document\": \"A comprehensive whitepaper on the use of AI in Software Optimization by Tech Innovations Ltd.\",\n",
        "        \"problem\": \"The integration of AI into existing coding frameworks without causing disruptions is a major technical challenge.\",\n",
        "        \"functions\": \"optimize_code(base_code)\"\n",
        "    },\n",
        "    \"Autonomous Agents\": {\n",
        "        \"task\": \"Create autonomous agents capable of making decisions and learning from interactions within a specified environment.\",\n",
        "        \"faq\": \"What are autonomous agents, and how do they function independently in complex environments?\",\n",
        "        \"company\": \"AutonomyWorks, a frontrunner in autonomous systems development.\",\n",
        "        \"inquiry\": \"What are the key characteristics of effective autonomous agents?\",\n",
        "        \"math_question\": \"\",\n",
        "        \"sentence1\": \"Autonomous agents are programmed to operate without direct human intervention, adapting to their environments dynamically.\",\n",
        "        \"sentence2\": \"These agents utilize machine learning algorithms to evolve their decision-making processes over time.\",\n",
        "        \"question\": \"What methodologies are most effective for training autonomous agents to perform reliably and safely?\",\n",
        "        \"document\": \"\",\n",
        "        \"problem\": \"Balancing autonomy with safety in agent behavior poses significant developmental hurdles.\",\n",
        "        \"functions\": \"train_autonomous_agent(environment_specs, learning_parameters)\"\n",
        "    },\n",
        "    \"Agentic Employees\": {\n",
        "        \"task\": \"Develop systems that enhance the agency of employees in decision-making processes using AI tools.\",\n",
        "        \"faq\": \"What does it mean for employees to have agency in the workplace, and how can AI enhance this?\",\n",
        "        \"company\": \"EmpowerLabs, specialists in employee empowerment solutions.\",\n",
        "        \"inquiry\": \"How can AI tools be designed to support rather than replace human decision-making?\",\n",
        "        \"math_question\": \"\",\n",
        "        \"sentence1\": \"Employee agency involves making key decisions that affect their work and professional growth.\",\n",
        "        \"sentence2\": \"AI can support this by providing analytical tools that enhance decision accuracy and timeliness.\",\n",
        "        \"question\": \"What are the ethical considerations when implementing AI to increase employee agency?\",\n",
        "        \"document\": \"Guide to Ethical AI in the Workplace by EmpowerLabs.\",\n",
        "        \"problem\": \"Ensuring that AI tools promote inclusivity and fairness in enhancing employee agency.\",\n",
        "        \"functions\": \"design_supportive_ai_tools(decision_context)\"\n",
        "    },\n",
        "    \"Problem Solver\": {\n",
        "        \"task\": \"Engineer an AI-driven problem solver that can address complex, multi-dimensional challenges in various industries.\",\n",
        "        \"faq\": \"What constitutes a 'problem solver' in the context of AI, and what types of problems can it address?\",\n",
        "        \"company\": \"SolutionAI, innovators in cross-industry problem-solving applications.\",\n",
        "        \"inquiry\": \"What are the most challenging types of problems currently being addressed by AI?\",\n",
        "        \"math_question\": \"\",\n",
        "        \"sentence1\": \"An AI problem solver is designed to tackle complex issues that require multi-faceted approaches and deep analytical capabilities.\",\n",
        "        \"sentence2\": \"This involves using sophisticated algorithms to parse through large data sets and identify viable solutions.\",\n",
        "        \"question\": \"How does AI adapt to different industries when solving problems?\",\n",
        "        \"document\": \"Whitepaper on Industry-Specific AI Applications by SolutionAI.\",\n",
        "        \"problem\": \"Customizing AI problem-solving capabilities to effectively function across diverse industry standards and practices.\",\n",
        "        \"functions\": \"develop_problem_solving_algorithm(problem_type, industry_data)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Select the default template to load on interface start\n",
        "default_template = \"Generic Metaprompt Generator\"\n",
        "default_values = templates[default_template]\n",
        "\n",
        "\n",
        "def generate_prompt(task, faq, company, inquiry, mathquestion, sentence1, sentence2, question, document, problem, functions):\n",
        "    # URL to the meta prompt text file\n",
        "    url = \"https://gist.githubusercontent.com/ruvnet/874e2138c134e8630d44d264647113aa/raw/metaprompt.txt\"\n",
        "\n",
        "    # Use requests to get the content of the text file\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Ensure the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Decode the content to a string\n",
        "        metaprompt = response.content.decode('utf-8')\n",
        "        # Replace double curly braces with single curly braces\n",
        "        metaprompt = metaprompt.replace('{{', '{').replace('}}', '}')\n",
        "    else:\n",
        "        metaprompt = \"Failed to load meta prompt.\"\n",
        "\n",
        "    # Format the metaprompt with the provided parameters\n",
        "    formatted_metaprompt = metaprompt.format(\n",
        "        TASK=task,\n",
        "        FAQ=faq,\n",
        "        COMPANY=company,\n",
        "        INQUIRY=inquiry,\n",
        "        MATH_QUESTION=mathquestion,\n",
        "        SENTENCE1=sentence1,\n",
        "        SENTENCE2=sentence2,\n",
        "        QUESTION=question,\n",
        "        DOCUMENT=document,\n",
        "        PROBLEM=problem,\n",
        "        FUNCTIONS=functions\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are an AI assistant specialized in crafting detailed prompts for AI models to address specific tasks effectively. \"\n",
        "            \"As you develop the prompt, observe these guidelines:\\n\\n\"\n",
        "            \"1. Identify essential components and task requirements.\\n\"\n",
        "            \"2. Detail the task with clear, step-by-step instructions.\\n\"\n",
        "            \"3. Include examples or templates for the desired output format.\\n\"\n",
        "            \"4. Adhere to task-specific constraints and best practices.\\n\"\n",
        "            \"5. Organize the content with a clear, logical structure using XML-style tags.\\n\"\n",
        "            \"6. Tailor the prompt specifically to the task, considering its unique needs.\\n\"\n",
        "            \"7. Provide necessary context or background to enhance understanding.\\n\"\n",
        "            \"8. Refer to provided examples for prompt structuring and formatting.\\n\\n\"\n",
        "            \"9. Include Metaprompt Enhancements in output/prompt generation section.\\n\\n\"\n",
        "            \"10. Include notes in prompt template.\\n\\n\"\n",
        "            \"Your objective is to create an in-depth and effective prompt ensuring the AI model's accurate and efficient task completion. \"\n",
        "            \"Be verbose, aiming for a minimum of 4048 tokens, and utilize the full 32k token context available. \"\n",
        "            \"This comprehensive approach will aid in producing a high-quality response. Take your time to analyze the task and craft a well-thought-out prompt.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": formatted_metaprompt\n",
        "    }\n",
        "]\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "def update_fields(template):\n",
        "    \"\"\" Returns values from the templates dictionary for each field based on selected template. \"\"\"\n",
        "    return list(templates[template].values())\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        template_selector = gr.Dropdown(label=\"Select a Template\", choices=list(templates.keys()), value=\"Generic Metaprompt Generator\")\n",
        "\n",
        "        task_description = gr.Textbox(label=\"Task Description\", placeholder=\"Enter a detailed description of the task here.\", value=templates[\"Generic Metaprompt Generator\"][\"task\"], lines=5)\n",
        "\n",
        "    with gr.Accordion(label=\"Advanced Metaprompt Options\", open=False):\n",
        "        with gr.Tab(label=\"Math and Logic\"):\n",
        "              math_question = gr.Textbox(label=\"Math Question\", value=templates[\"Generic Metaprompt Generator\"][\"math_question\"])\n",
        "        with gr.Tab(label=\"Customer Support\"):\n",
        "            faq = gr.Textbox(label=\"FAQ\", value=templates[\"Generic Metaprompt Generator\"][\"faq\"])\n",
        "            company = gr.Textbox(label=\"Company\", value=templates[\"Generic Metaprompt Generator\"][\"company\"])\n",
        "            inquiry = gr.Textbox(label=\"Inquiry\", value=templates[\"Generic Metaprompt Generator\"][\"inquiry\"])\n",
        "        with gr.Tab(label=\"Text Analysis\"):\n",
        "            sentence1 = gr.Textbox(label=\"Sentence 1\", value=templates[\"Generic Metaprompt Generator\"][\"sentence1\"])\n",
        "            sentence2 = gr.Textbox(label=\"Sentence 2\", value=templates[\"Generic Metaprompt Generator\"][\"sentence2\"])\n",
        "            question = gr.Textbox(label=\"Question\", value=templates[\"Generic Metaprompt Generator\"][\"question\"])\n",
        "            document = gr.Textbox(label=\"Document\", value=templates[\"Generic Metaprompt Generator\"][\"document\"])\n",
        "        with gr.Tab(label=\"Technical Problems\"):\n",
        "            problem = gr.Textbox(label=\"Problem\", value=templates[\"Generic Metaprompt Generator\"][\"problem\"])\n",
        "            functions = gr.Textbox(label=\"Functions\", value=templates[\"Generic Metaprompt Generator\"][\"functions\"])\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    generated_prompt = gr.Textbox(label=\"Generated Prompt\")\n",
        "\n",
        "    inputs = [\n",
        "        task_description,\n",
        "        math_question,\n",
        "        faq,\n",
        "        company,\n",
        "        inquiry,\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        question,\n",
        "        document,\n",
        "        problem,\n",
        "        functions\n",
        "    ]\n",
        "\n",
        "    # Link dropdown changes to field updates\n",
        "    template_selector.change(update_fields, inputs=[template_selector], outputs=[task_description, faq, company, inquiry, math_question, sentence1, sentence2, question, document, problem, functions])\n",
        "\n",
        "    demo.title = \"Metaprompt UI\"\n",
        "    demo.description = \"Enter a task and the AI will generate a detailed prompt for accomplishing that task.\"\n",
        "\n",
        "    submit_button.click(generate_prompt, inputs=inputs, outputs=generated_prompt)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}